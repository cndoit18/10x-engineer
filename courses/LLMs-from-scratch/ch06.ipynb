{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e263a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c21028",
   "metadata": {},
   "source": [
    "由于类别分类并不是均匀分布的，需要做一些额外处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5063a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Label\"].value_counts())\n",
    "\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from spam import SpamDataset\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f710531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import ModelArgs, GPTModel\n",
    "import requests\n",
    "from dataclasses import replace\n",
    "import os\n",
    "from safetensors.torch import load_file\n",
    "from weight_loading_hf_safetensors import load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = ModelArgs(\n",
    "    vocab_size=50257,\n",
    "    context_length=1024,\n",
    "    drop_rate=0.0,\n",
    "    qkv_bias=True,\n",
    ")\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12, \"inter_dim\": 3072},\n",
    "    \"gpt2-large\": {\n",
    "        \"emb_dim\": 1280,\n",
    "        \"n_layers\": 36,\n",
    "        \"n_heads\": 20,\n",
    "        \"inter_dim\": 5120,\n",
    "    },\n",
    "    \"gpt2-xl\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25, \"inter_dim\": 6400},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-xl\"\n",
    "MODEL_CONFIG = replace(BASE_CONFIG, **model_configs[CHOOSE_MODEL])\n",
    "\n",
    "url = f\"https://huggingface.co/openai-community/{CHOOSE_MODEL}/resolve/main/model.safetensors\"\n",
    "output_file = f\"model-{CHOOSE_MODEL}.safetensors\"\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "state_dict = load_file(output_file)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt = GPTModel(MODEL_CONFIG)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "load_weights_into_gpt(gpt, state_dict)\n",
    "gpt.to(device)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_train import (\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text,\n",
    "    generate,\n",
    ")\n",
    "\n",
    "\n",
    "text = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(text, tokenizer).to(device),\n",
    "    max_new_tokens=23,\n",
    "    context_size=MODEL_CONFIG.context_length,\n",
    "    top_k=25,\n",
    "    temperature=1.4,\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eacb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in gpt.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "gpt.out_head = torch.nn.Linear(in_features=MODEL_CONFIG.emb_dim, out_features=2).to(\n",
    "    device\n",
    ")\n",
    "for param in gpt.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in gpt.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, gpt, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, device, num_batches=10)\n",
    "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91dc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "with (\n",
    "    torch.no_grad()\n",
    "):  # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, gpt, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter\n",
    "):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[\n",
    "                0\n",
    "            ]  # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(\n",
    "                    f\"Ep {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        print(f\"Training accuracy: {train_accuracy * 100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    gpt,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=50,\n",
    "    eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, gpt, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, device, num_batches=10)\n",
    "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(\n",
    "    text, model, tokenizer, device, max_length=None, pad_token_id=50256\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.emb.pos_emb.weight.shape[0]\n",
    "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
    "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[: min(max_length, supported_context_length)]\n",
    "    assert max_length is not None, (\n",
    "        \"max_length must be specified. If you want to use the full model context, \"\n",
    "        \"pass max_length=model.pos_emb.weight.shape[0].\"\n",
    "    )\n",
    "    assert max_length <= supported_context_length, (\n",
    "        f\"max_length ({max_length}) exceeds model's supported context length ({supported_context_length}).\"\n",
    "    )\n",
    "    # Alternatively, a more robust version is the following one, which handles the max_length=None case better\n",
    "    # max_len = min(max_length,supported_context_length) if max_length else supported_context_length\n",
    "    # input_ids = input_ids[:max_len]\n",
    "\n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(\n",
    "        0\n",
    "    )  # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\"\n",
    "\n",
    "\n",
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    classify_review(text_1, gpt, tokenizer, device, max_length=train_dataset.max_length)\n",
    ")\n",
    "text_2 = \"Hey, just wanted to check if we're still on for dinner tonight? Let me know!\"\n",
    "\n",
    "print(\n",
    "    classify_review(text_2, gpt, tokenizer, device, max_length=train_dataset.max_length)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
