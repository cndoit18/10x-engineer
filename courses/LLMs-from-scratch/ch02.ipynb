{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba013443",
   "metadata": {},
   "source": [
    "分词就是将句子、段落、文章这种长文本，分解为以字词为单位的数据结构，方便后续的处理分析工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 基于正则表达式进行分词\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, unk_token=\"<|unk|>\", endof_token=\"<|endoftext|>\"):\n",
    "        if vocab.get(unk_token) is None:\n",
    "            raise ValueError(f\"Vocabulary must contain unk_token '{unk_token}'\")\n",
    "        if vocab.get(endof_token) is None:\n",
    "            raise ValueError(f\"Vocabulary must contain endof_token '{endof_token}'\")\n",
    "\n",
    "        self._unk_token = unk_token\n",
    "        self._endof_token = endof_token\n",
    "        self._str_to_int = vocab\n",
    "        self._int_to_str = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [items.strip() for items in preprocessed if items.strip()]\n",
    "        return [\n",
    "            self._str_to_int.get(item, self._str_to_int[self._unk_token])\n",
    "            for item in preprocessed\n",
    "        ]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join(self._int_to_str[token] for token in tokens)\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r\"\\1\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bb663",
   "metadata": {},
   "source": [
    "我们使用这个文件来构建我们的词表，并使用它来实例化一个分词器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf65966",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"asserts/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total length of the text:\", len(raw_text))\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [items.strip() for items in preprocessed if items.strip()]\n",
    "print(\"Total length of the preprocessed text:\", len(preprocessed))\n",
    "\n",
    "preprocessed.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(sorted(set(preprocessed)))}\n",
    "print(\"Total length of the vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab)\n",
    "text1 = \"Hello, do you like tea\"\n",
    "text2 = \"\"\"\"It's the last he painted, you know, \"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(\"<|endoftext|>\".join([text1, text2]))\n",
    "print(ids)\n",
    "text = tokenizer.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e3da9",
   "metadata": {},
   "source": [
    "接下来我们使用 gpt2 的 tokenizer 来进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a98df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text1 = \"Hello, do you like tea\"\n",
    "text2 = \"\"\"\"It's the last he painted, you know, \"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "idx = tokenizer.encode(\n",
    "    \"<|endoftext|>\".join([text1, text2]), allowed_special={\"<|endoftext|>\"}\n",
    ")\n",
    "print(idx)\n",
    "text = tokenizer.decode(idx)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
