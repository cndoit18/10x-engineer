{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba013443",
   "metadata": {},
   "source": [
    "分词就是将句子、段落、文章这种长文本，分解为以字词为单位的数据结构，方便后续的处理分析工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 基于正则表达式进行分词\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, unk_token=\"<|unk|>\", endof_token=\"<|endoftext|>\"):\n",
    "        if vocab.get(unk_token) is None:\n",
    "            raise ValueError(f\"Vocabulary must contain unk_token '{unk_token}'\")\n",
    "        if vocab.get(endof_token) is None:\n",
    "            raise ValueError(f\"Vocabulary must contain endof_token '{endof_token}'\")\n",
    "\n",
    "        self._unk_token = unk_token\n",
    "        self._endof_token = endof_token\n",
    "        self._str_to_int = vocab\n",
    "        self._int_to_str = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [items.strip() for items in preprocessed if items.strip()]\n",
    "        return [\n",
    "            self._str_to_int.get(item, self._str_to_int[self._unk_token])\n",
    "            for item in preprocessed\n",
    "        ]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join(self._int_to_str[token] for token in tokens)\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r\"\\1\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bb663",
   "metadata": {},
   "source": [
    "我们使用这个文件来构建我们的词表，并使用它来实例化一个分词器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf65966",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"asserts/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total length of the text:\", len(raw_text))\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [items.strip() for items in preprocessed if items.strip()]\n",
    "print(\"Total length of the preprocessed text:\", len(preprocessed))\n",
    "\n",
    "preprocessed.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "vocab = {token: integer for integer, token in enumerate(sorted(set(preprocessed)))}\n",
    "print(\"Total length of the vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab)\n",
    "text1 = \"Hello, do you like tea\"\n",
    "text2 = \"\"\"\"It's the last he painted, you know, \"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(\"<|endoftext|>\".join([text1, text2]))\n",
    "print(ids)\n",
    "text = tokenizer.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e3da9",
   "metadata": {},
   "source": [
    "接下来我们使用 gpt2 的 tokenizer 来进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a98df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text1 = \"Hello, do you like tea\"\n",
    "text2 = \"\"\"\"It's the last he painted, you know, \"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "idx = tokenizer.encode(\n",
    "    \"<|endoftext|>\".join([text1, text2]), allowed_special={\"<|endoftext|>\"}\n",
    ")\n",
    "print(idx)\n",
    "text = tokenizer.decode(idx)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e26f63",
   "metadata": {},
   "source": [
    "接下来我们来实现一个简单的数据集，用于训练模型。\n",
    "对于 GPT 来说，它需要的输入是连续的字词，而输出是下一个字词。\n",
    "所以我们需要以此构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "context_size = 10\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9257d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "我们用 DataSet 和 DataLoader 来实现这个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        # 在初始化的时候按照上面的思路做拆分\n",
    "        self._chunks = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = tuple(token_ids[i : i + max_length])\n",
    "            output_chunk = tuple(token_ids[i + 1 : i + max_length + 1])\n",
    "            self._chunks.append((input_chunk, output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._chunks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._chunks[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf06572c",
   "metadata": {},
   "source": [
    "我们模拟一下数据集的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPTDataset(raw_text, tokenizer, max_length=10, stride=1)\n",
    "for i in dataset[:3]:\n",
    "    print(tokenizer.decode(i[0]), \"---->\", tokenizer.decode(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3abbd",
   "metadata": {},
   "source": [
    "我们做一个函数来创建一个 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    text,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    return DataLoader(\n",
    "        GPTDataset(text, tokenizer, max_length=max_length, stride=stride),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=1, max_length=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "input, output = first_batch\n",
    "print(input, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
